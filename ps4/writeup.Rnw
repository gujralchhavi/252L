\documentclass{article}

\usepackage{lineno}
\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry}

\newcommand\ddfrac[2]{\frac{\displaystyle #1}{\displaystyle #2}}

\linenumbers

\title{Problem Set 4}
\author{Carrie Kathlyn Townley Flores, Filipe Recch, Kaylee Tuggle Matheny, \\ Klint Kanopka, Kritphong Mongkhonvanit \\ EDUC 252L}

<<echo=FALSE,warning=FALSE,message=FALSE,error=FALSE>>=
knitr::opts_chunk$set(cache=TRUE, echo = FALSE, fig.pos="H",fig.height=5, fig.width=8, fig.align = "center")
library(mirt)
@

\begin{document}
\maketitle

\begin{enumerate}
	\item
	\item
	\item
<<message=FALSE,results='hide'>>=
set.seed(12311)
read.table("emp-reading-3pl-gpcm.txt",header=TRUE)->resp
resp[rowSums(is.na(resp))==0,]->resp
resp[1:5000,]->resp
#just the constructed response items
grep("^cr",names(resp))->index
resp[,index]->resp

##Let's look at different dichotomizations for just a single item, #2

Theta <- matrix(seq(-4,4, by = .1))
colors <- c("black", "red", "green", "blue")
apply_cutoff <- function(min_score) {
    resp->tmp
    ifelse(tmp[,2]>=min_score,1,0)->tmp[,2]
    mirt(tmp, itemtype="gpcmIRT",1,SE=TRUE)
}
dic <- c(	mirt(resp, itemtype="gpcmIRT",1,SE=TRUE),
		sapply(1:3, apply_cutoff) )

@
	\begin{enumerate}
		\item
		 What is the effect of dichotomizing low (e.g., responses of 1 and 2 become 1) versus high (e.g., responses of 0 and 1 become 0) on the central tendency and spread of information curves. \\
		 
		The information curves will peak at the cutoff points, therefore if we dichotomize low we will have more information about lower levels of $\theta$. The same behaviour is the same if we decide to dichotomize at the higher points. Finally, the peaks appear to spread uniformly across the x-axis from the cut off points. 

<<>>=
for (i in 1:4) {
    extr.2 <- extract.item(dic[[i]], 2)
    info.tmp <- iteminfo(extr.2, Theta)
    (if (i==1) plot else lines)(Theta,info.tmp,col=colors[i],type="l", xlab=expression(theta))
}

legend(	x="topright",
	col=c("black","red","green","blue"),
	lty=c(1,1,1,1),
	legend=c(	"polytomous",
			"cutoff at 1",
			"cutoff at 2",
			"cutoff at 3"))
@

		\item
		How do standard errors for theta compare when we estimate the GPCM on the data versus low and high dichotimizations? \\
		
		The standard error is smallest when the cutoff point is the closest to the maxima of the polytomous information curve. Therefore, we will have lower levels of SE when we have more information across $\theta$. In this case, it happens when the cutoff is at two.

<<>>=
for (i in 1:4) {
    extr.2 <- extract.item(dic[[i]], 2)
    SE.tmp <- 1/sqrt(iteminfo(extr.2, Theta))
    (if (i==1) plot else lines)(Theta,SE.tmp,col=colors[i],type="l", xlab=expression(theta))
}

legend(x="bottomright",
	col=c("black","red","green","blue"),
	lty=c(1,1,1,1),
	legend=c("polytomous",
			"cutoff at 1",
			"cutoff at 2",
			"cutoff at 3"))
@


<<message=FALSE,fig.width=6>>=
# get SEs for dichotomous items
se <- sapply(	2:length(dic),
		function(i) coef(dic[[i]],printSE=TRUE)[[2]][2,][["b1"]] )
barplot(se,xlab="Cutoff Score",ylab="Standard Error",names.arg=paste(1:3))
@
	\end{enumerate}

	\item
	Here's an implementation of the EM algorithm:
	
<<echo=TRUE>>=
EM <- function(PA,PB,sets,N,pA,pB){
  
  #construct flip matrix
  flips <- matrix(NA,sets,N)
  for (i in 1:sets){
    prob <- ifelse(runif(1)<0.5,PA,PB)
    for (j in 1:N){
      flips[i,j] <- ifelse(runif(1)<prob,1,0)
    }
  }
  
  dpA <- dpB <- 1
  thresh <- 0.0001
  
  while (abs((dpA+dpB)/2)>= thresh){
      
    coins <- c(0,0,0,0)
    for (i in 1:sets){
      wA <- pA^rowSums(flips)[i]*(1-pA)^(N-rowSums(flips)[i])
      wB <- pB^rowSums(flips)[i]*(1-pB)^(N-rowSums(flips)[i])
      wA <- wA/(wA+wB)
      wB <- wB/(wA+wB)
      coins[1] <- coins[1] + wA*rowSums(flips)[i]
      coins[2] <- coins[2] + wA*(N-rowSums(flips)[i])
      coins[3] <- coins[3] + wB*rowSums(flips)[i]
      coins[4] <- coins[4] + wB*(N-rowSums(flips)[i])
    }
    
    pAold <- pA
    pBold <- pB
  
    pA <- coins[1]/(coins[1]+coins[2])
    pB <- coins[3]/(coins[3]+coins[4])
    
    dpA <- pA - pAold
    dpB <- pB - pBold
  }
  return(c(pA,pB))
}
@
  From this, we can ask some questions about how sensitive it is to wiggling of the parameters.  First lets look at the sensitivity of the EM algorithm to the initial probability estimates:
  
<<>>=
temp <- 
p <-seq(.1, .8, length.out=100)
aresults <- matrix(NA,100,3)
for (n in 1:100){
  temp <- matrix(NA,100,2)
  a <- p[n]
  for (i in 1:100){
    t <- EM(.3,.7,5,10,a,.8)
    temp[i,1] <- (t[1]-.3)^2
    temp[i,2] <- (t[2]-.7)^2
  }
  aresults[n,1] <- 0.8-p[n]
  aresults[n,2] <- mean(temp[,1])
  aresults[n,3] <- mean(temp[,2])
}

plot(aresults[,1],aresults[,2],main='First Probability Estimation', xlab='Separation Between Starting Estimates', ylab='Mean Square Error')
plot(aresults[,1],aresults[,3],main='Second Probability Estimation', xlab='Separation Between Starting Estimates', ylab='Mean Square Error')

@
  Looking at the two probability estimates, it doesn't appear that there's a relationship between the separation of the starting values and the mean square error.  Next we examine the error in the estimates and the choice of the true parameters:
  
<<>>=

P <-seq(0, .8, length.out=100)
bresults <- matrix(NA,100,3)
for (n in 1:100){
  temp <- matrix(NA,100,2)
  a <- P[n]
  for (i in 1:100){
    t <- EM(a,.8,5,10,.45,.55)
    temp[i,1] <- (t[1]-a)^2
    temp[i,2] <- (t[2]-.8)^2
  }
  bresults[n,1] <- 0.8-a
  bresults[n,2] <- mean(temp[,1])
  bresults[n,3] <- mean(temp[,2])
}

plot(bresults[,1],bresults[,2],main='First Probability Estimation', xlab='Separation Between True Parameters', ylab='Mean Square Error')
plot(bresults[,1],bresults[,3],main='Second Probability Estimation', xlab='Separation Between True Parameters', ylab='Mean Square Error')

@
  
  Looking at these plots, mean square error increases as the separation between the two parameters increases.  This makes sense, becuase the farther apart the two probabilities are, the more "work" the algorithm has to do to discern between what coin generated which data set.  Next we look at the number of coin flips per set (with the number of sets held constant at 5):
  
<<>>=
cresults <- matrix(NA,100,3)
for (N in 1:100){
  temp <- matrix(NA,100,2)
  for (i in 1:100){
    t <- EM(.3,.7,5,N,.45,.55)
    temp[i,1] <- (t[1]-.3)^2
    temp[i,2] <- (t[2]-.7)^2
  }
  cresults[N,1] <- N
  cresults[N,2] <- mean(temp[,1])
  cresults[N,3] <- mean(temp[,2])
}
plot(cresults[,1],cresults[,2],main='First Probability Estimation', xlab='Number of Coin Flips', ylab='Mean Square Error')
plot(cresults[,1],cresults[,3],main='Second Probability Estimation', xlab='Number of Coin Flips', ylab='Mean Square Error')

@
  Here it's clear that more coin flips per set quickly reduces the mean square error, but the effect tapers off quite rapidly.  Next we look at the effect of the number of sets of flips (with flips per set held constant at 10).
  
<<>>=
dresults <- matrix(NA,100,3)
for (N in 1:100){
  temp <- matrix(NA,100,2)
  for (i in 1:100){
    t <- EM(.3,.7,N,10,.45,.55)
    temp[i,1] <- (t[1]-.3)^2
    temp[i,2] <- (t[2]-.7)^2
  }
  dresults[N,1] <- N
  dresults[N,2] <- mean(temp[,1])
  dresults[N,3] <- mean(temp[,2])
}

plot(dresults[,1],dresults[,2],main='First Probability Estimation', xlab='Number of Sets of Flips', ylab='Mean Square Error')
plot(dresults[,1],dresults[,3],main='Second Probability Estimation', xlab='Number of Sets of Flips', ylab='Mean Square Error')
@

  Here we see that the number of sets of flips has an effect similar to the number of flips per set - quickly reducing the mean square error, but also tapering off relatively quickly.

\end{enumerate}

\end{document}
